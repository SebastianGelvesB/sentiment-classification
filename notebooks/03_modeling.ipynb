{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0950b5",
   "metadata": {},
   "source": [
    "# **Modelado**\n",
    "\n",
    "En este notebook hacemos todo el proceso de fine-tuning de los 3 modelos diferente y evaluamos su performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137eb68c",
   "metadata": {},
   "source": [
    "# 0. Librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "16faf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4230abd",
   "metadata": {},
   "source": [
    "# 1. Datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cacde8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/processed/train_encoded.csv')\n",
    "df_val = pd.read_csv('../data/processed/validation_encoded.csv')\n",
    "df_test = pd.read_csv('../data/processed/test_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7c52ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to everyone hoarding rice who until now doesnâ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if your going to eat they have complementary w...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>watch this if you are one of those idiots who ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we need to have a risk management system more ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>markets plunge puts pension freedoms to the te...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment\n",
       "0  to everyone hoarding rice who until now doesnâ...          0\n",
       "1  if your going to eat they have complementary w...          4\n",
       "2  watch this if you are one of those idiots who ...          0\n",
       "3  we need to have a risk management system more ...          3\n",
       "4  markets plunge puts pension freedoms to the te...          3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1e39a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meanwhile a villager of quenching her thirsty ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us ethanol and biodiesel trends in prices and ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>today my husband came home from the supermarke...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so theres no cure for a virus than can be kill...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like the good new yorker i am i talked myself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment\n",
       "0  meanwhile a villager of quenching her thirsty ...          2\n",
       "1  us ethanol and biodiesel trends in prices and ...          3\n",
       "2  today my husband came home from the supermarke...          4\n",
       "3  so theres no cure for a virus than can be kill...          0\n",
       "4  like the good new yorker i am i talked myself ...          4"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f780fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when i couldnt find hand sanitizer at fred mey...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>find out how you can protect yourself and love...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buying hits city as anxious shoppers stock up ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one week everyone buying baby milk powder the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment\n",
       "0  trending new yorkers encounter empty supermark...          0\n",
       "1  when i couldnt find hand sanitizer at fred mey...          3\n",
       "2  find out how you can protect yourself and love...          4\n",
       "3  buying hits city as anxious shoppers stock up ...          1\n",
       "4  one week everyone buying baby milk powder the ...          2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348e4cd",
   "metadata": {},
   "source": [
    "# 2.  Modelado General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea6bea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Funciones \n",
    "\n",
    "## 1. Tokenization\n",
    "\n",
    "# Definimos función para tokenizar data. Seteamos max_length=128 de acuerdo a lo analizado en el EDA.\n",
    "def tokenize_data(data, tokenizer):\n",
    "    tokenized = tokenizer(data.astype(str).tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Definimos función para tokenizar los datasets de train, val y test.\n",
    "def tokenize_datasets(df_train, df_val, df_test, tokenizer):\n",
    "    train_tokenized = tokenize_data(df_train['OriginalTweet'], tokenizer)\n",
    "    val_tokenized = tokenize_data(df_val['OriginalTweet'], tokenizer)\n",
    "    test_tokenized = tokenize_data(df_test['OriginalTweet'], tokenizer)\n",
    "    \n",
    "    return train_tokenized, val_tokenized, test_tokenized\n",
    "\n",
    "\n",
    "## 2. Creción de TensorDatasets\n",
    "\n",
    "# Función para convertir labels a tensores\n",
    "def convert_to_tensor(df_train, df_val, df_test):\n",
    "    labels_train = torch.tensor(df_train['Sentiment'])\n",
    "    labels_val = torch.tensor(df_val['Sentiment'])\n",
    "    labels_test = torch.tensor(df_test['Sentiment'])\n",
    "\n",
    "    return labels_train, labels_val, labels_test\n",
    "\n",
    "# Función para crear TensorDatasets a partir de los tweets tokenizados y el tensor de las etiquetas\n",
    "def create_tensordatasets(train_tokenized, labels_train, val_tokenized, labels_val, test_tokenized, labels_test):\n",
    "    train_dataset = TensorDataset(train_tokenized['input_ids'], train_tokenized['attention_mask'], labels_train)\n",
    "    val_dataset = TensorDataset(val_tokenized['input_ids'], val_tokenized['attention_mask'], labels_val)\n",
    "    test_dataset = TensorDataset(test_tokenized['input_ids'], test_tokenized['attention_mask'], labels_test)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "## 3. Creación de DataLoaders\n",
    "\n",
    "# Función para crear los DataLoaders de cada dataset\n",
    "def create_dataloader(train_dataset,val_dataset,test_dataset, batch_size):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "## 4. Calculamos Función de pérdida\n",
    "\n",
    "# Función para calcular función de pérdida\n",
    "def calculate_loss_fn(labels_train, device):\n",
    "    # Calculamos los pesos para ponderar la función de pérdida para mitigar el desbalance de clases en el dataset original\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_train.numpy()), y=labels_train.numpy())\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    # Definimos la función de pérdida\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "## 5. Entrenamiento del modelo\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, optimizer, loss_fn,  train_dataloader, val_dataloader, epochs, device):\n",
    "\n",
    "    # definimos scheduler\n",
    "    scheduler = get_scheduler(\n",
    "        name=\"linear\", \n",
    "        optimizer=optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "\n",
    "    # Entrenamiento\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Seteamos modo training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=\"Entrenando\"):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Actualizamos métricas\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_loss = train_loss / len(train_dataloader)\n",
    "        acc = correct / total\n",
    "        print(f\"Train Loss: {avg_loss:.4f} | Train Accuracy: {acc:.4f}\")\n",
    "\n",
    "        # Validación\n",
    "        # Seteamos modo de evaluación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[2].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_acc = correct / total\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "## 6. Guardamos el modelo y el tokenizer\n",
    "\n",
    "# Función para guardar modelo y tokenizer\n",
    "def save_model_tokenizer(model, tokenizer,save_path):\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "\n",
    "\n",
    "## 7. Evaluación del modelo en el dataset de test\n",
    "\n",
    "# Función para guardar los experimentos realizados\n",
    "def save_experiment_results(model_name, hyperparams, y_test, y_test_pred, encoder, output_path):\n",
    "    # Calculamos métricas\n",
    "    acc = accuracy_score(y_test, y_test_pred)\n",
    "    f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "    report = classification_report(y_test, y_test_pred, target_names=encoder.categories_[0], output_dict=True)\n",
    "\n",
    "    # Creamos registro del experimento\n",
    "    experiment = {\n",
    "        \"modelo\": model_name,\n",
    "        \"hiperparametros\": hyperparams,\n",
    "        \"accuracy\": round(acc, 4),\n",
    "        \"f1_macro\": round(f1_macro, 4),\n",
    "        \"reporte_clasificacion\": report\n",
    "    }\n",
    "\n",
    "    # Guardamos en archivo .json (agrega múltiples experimentos)\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = []\n",
    "\n",
    "    results.append(experiment)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"### Resultados del experimento guardados en {output_path}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, model_name,hyperparams, test_dataloader, device, path_encoder,output_path):\n",
    "    # Cargamos el encoder\n",
    "    encoder = joblib.load(path_encoder)\n",
    "\n",
    "    # Seteamos modo de evaluación\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print('#### Resultados de Evaluacion #### \\n\\n')\n",
    "    print(classification_report(all_labels, all_preds, target_names=encoder.categories_[0]))\n",
    "\n",
    "    print('\\n')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=encoder.categories_[0])\n",
    "    disp.plot(xticks_rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Guardamos métricas del experimento\n",
    "    save_experiment_results(\n",
    "    model_name,\n",
    "    hyperparams,\n",
    "    y_test=all_labels,\n",
    "    y_test_pred=all_preds,\n",
    "    encoder=encoder,\n",
    "    output_path = output_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a1d94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 8. Función para ejecutar y orquestar todo el pipeline de entrenamiento\n",
    "\n",
    "def execute_modeling_pipeline(df_train, df_val, df_test, model_name,  lr, weight_decay, batch_size, epochs, save_path, device, save_model=False):\n",
    "    \n",
    "    print(f'#### Pipeline para el modelo {model_name} iniciado #### \\n')\n",
    "\n",
    "    ## 1. Tokenization\n",
    "\n",
    "    # Inicializamos el tokenizer para el modelo seleccionado\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenizamos los datasets\n",
    "    train_tokenized, val_tokenized, test_tokenized = tokenize_datasets(df_train, df_val, df_test, tokenizer)\n",
    "\n",
    "    print('### Proceso de Tokenization finalizado ### \\n')\n",
    "\n",
    "\n",
    "    ## 2. Creación de TensorDataset, objetos necesarios para trabajar en pytorch\n",
    "\n",
    "    # Convertimos la etiqueta, Sentiment, en tensores\n",
    "    labels_train, labels_val, labels_test = convert_to_tensor(df_train, df_val, df_test)\n",
    "\n",
    "    # Creamos los TensorDataset a partir de los tweets tokenizados y el tensor de las etiquetas\n",
    "    train_dataset, val_dataset, test_dataset = create_tensordatasets(train_tokenized, labels_train, val_tokenized, labels_val, test_tokenized, labels_test)\n",
    "\n",
    "\n",
    "    ## 3. Creación de DataLoaders\n",
    "\n",
    "    # Definimos los dataloaders para entregar los TensorDataset a pytorch por batches\n",
    "    train_dataloader, val_dataloader, test_dataloader = create_dataloader(train_dataset,val_dataset,test_dataset, batch_size)\n",
    "\n",
    "    print('### Dataloaders Creados ### \\n')\n",
    "\n",
    "\n",
    "    ## 4. Función de pérdida\n",
    "\n",
    "    # Calculamos la función de pérdida ponderada\n",
    "    loss_fn =  calculate_loss_fn(labels_train, device)\n",
    "\n",
    "    print('### Función de pérdida calculada ### \\n')\n",
    "\n",
    "\n",
    "    ## 5. Entrenamiento del modelo especificado\n",
    "\n",
    "    print(f'### Inicio de Fine-Tuning del modelo {model_name} ### \\n')\n",
    "    \n",
    "    # Definimos el modelo\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, return_dict=True)\n",
    "    model.to(device)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Definimos optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    train_model(model, optimizer, loss_fn,  train_dataloader, val_dataloader, epochs=epochs, device=device)\n",
    "\n",
    "    print(f'\\n\\n### Fin de Fine-Tuning del modelo {model_name} ### \\n')\n",
    "\n",
    "    ## 6. Guardamos el modelo entrenado para posterior evaluación en dataset de test\n",
    "\n",
    "    # Guardamos el estado actual del modelo y el tokenizer\n",
    "    if save_model:\n",
    "        save_model_tokenizer(model, tokenizer, save_path)\n",
    "\n",
    "        print(f'### Modelo finetuneado y tokenizer guardado en {save_path} ### \\n')\n",
    "\n",
    "    return model, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02cd37",
   "metadata": {},
   "source": [
    "# 3. Modelado de BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos parámetros del modelado\n",
    "hyperparams = {\n",
    "'model_name' : 'bert-base-uncased',\n",
    "'lr':3e-6,\n",
    "'weight_decay':1e-7,\n",
    "'batch_size' : 16,\n",
    "'epochs': 1,\n",
    "'save_path': \"../models/bert_sentiment_analysis_model\"\n",
    "}\n",
    "\n",
    "# Seteamos uso de gpu\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Ejecutamos el pipeline de entrenamiento\n",
    "model, test_dataloader = execute_modeling_pipeline(df_train, df_val, df_test, *hyperparams.values(), device, save_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el modelo BERT\n",
    "path_encoder= '../models/ordinal_encoder.pkl'\n",
    "output_path = '../outputs/resultados_experimentos.json'\n",
    "\n",
    "evaluate_model(model,hyperparams['model_name'], hyperparams, test_dataloader, device, path_encoder,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8a0e4",
   "metadata": {},
   "source": [
    "# 4. Modelado de RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos parámetros del modelado\n",
    "hyperparams = {\n",
    "'model_name' : 'roberta-base',\n",
    "'lr':3e-6,\n",
    "'weight_decay':1e-7,\n",
    "'batch_size' : 16,\n",
    "'epochs': 1,\n",
    "'save_path': \"../models/roberta_sentiment_analysis_model\"\n",
    "}\n",
    "\n",
    "# Seteamos uso de gpu\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Ejecutamos el pipeline de entrenamiento\n",
    "model, test_dataloader = execute_modeling_pipeline(df_train, df_val, df_test, *hyperparams.values(), device, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123bc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el modelo RoBERTa\n",
    "evaluate_model(model,hyperparams['model_name'], hyperparams, test_dataloader, device, path_encoder,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c12c66",
   "metadata": {},
   "source": [
    "# 5. Modelado de DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad13b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos parámetros del modelado\n",
    "hyperparams = {\n",
    "'model_name' : 'distilbert-base-uncased',\n",
    "'lr':3e-6,\n",
    "'weight_decay':1e-7,\n",
    "'batch_size' : 16,\n",
    "'epochs': 1,\n",
    "'save_path': \"../models/distilbert_sentiment_analysis_model\"\n",
    "}\n",
    "\n",
    "# Seteamos uso de gpu\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Ejecutamos el pipeline de entrenamiento\n",
    "model, test_dataloader = execute_modeling_pipeline(df_train, df_val, df_test, *hyperparams.values(), device, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4859ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el modelo DistilBERT\n",
    "evaluate_model(model,hyperparams['model_name'], hyperparams, test_dataloader, device, path_encoder,output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sent-nlp-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
